const { ImageAnnotatorClient } = require('@google-cloud/vision');
const { convert } = require('pdf2pic');
const fs = require('fs');
const path = require('path');
const { GoogleGenerativeAI } = require('@google/generative-ai');
const { db } = require('../database');
require('dotenv').config();

class VisionOCRService {
  constructor() {
    this.tempDir = './temp-images';
    this.ensureTempDir();
    
    // Initialize Google Cloud Vision
    // Handle both local (keyFilename) and production (JSON credentials) environments
    let visionConfig = {
      projectId: process.env.GOOGLE_CLOUD_PROJECT_ID
    };

    if (process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON) {
      // Production: Parse JSON credentials from environment variable
      try {
        const credentials = JSON.parse(process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON);
        visionConfig.credentials = credentials;
        console.log('üîë Using JSON credentials for Google Cloud Vision');
      } catch (error) {
        console.error('‚ùå Failed to parse GOOGLE_APPLICATION_CREDENTIALS_JSON:', error);
        throw error;
      }
    } else if (process.env.GOOGLE_APPLICATION_CREDENTIALS) {
      // Local development: Use keyFilename
      visionConfig.keyFilename = process.env.GOOGLE_APPLICATION_CREDENTIALS;
      console.log('üîë Using keyFilename for Google Cloud Vision');
    } else {
      throw new Error('No Google Cloud credentials found. Set either GOOGLE_APPLICATION_CREDENTIALS_JSON or GOOGLE_APPLICATION_CREDENTIALS');
    }

    this.visionClient = new ImageAnnotatorClient(visionConfig);
    
    // Initialize Gemini AI for text correction and content analysis
    this.genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
    this.model = this.genAI.getGenerativeModel({ model: 'gemini-2.5-flash' });
    if (!fs.existsSync(this.tempDir)) {
      fs.mkdirSync(this.tempDir, { recursive: true });
    }
  }

  ensureTempDir() {
    if (!fs.existsSync(this.tempDir)) {
      fs.mkdirSync(this.tempDir, { recursive: true });
    }
  }

  // Check if PDF is likely scanned (has very little text)
  isScannedPDF(extractedText) {
    if (!extractedText || extractedText.trim().length < 100) {
      return true;
    }
    
    // Check if text is mostly whitespace or special characters
    const meaningfulText = extractedText.replace(/\s+/g, '').replace(/[^a-zA-Z√Ä-·ªπ0-9]/g, '');
    return meaningfulText.length < 50;
  }

  // Convert PDF to images with high quality settings
  async convertPDFToImages(pdfPath, maxPages = null) {
    // Use environment variable or default to 50 pages (was 10)
    const pageLimit = maxPages || parseInt(process.env.MAX_PDF_PAGES) || 50;
    try {
      const options = {
        density: 300,           // High DPI for Vision API
        saveFilename: "page",
        savePath: this.tempDir,
        format: "png",
        quality: 100,
        width: 2000,           // Good resolution for Vision API
        graphicsmagick: true
      };

      console.log(`üì∑ Converting PDF to images for Vision API (limit: ${pageLimit} pages)...`);
      
      const convert = require('pdf2pic').fromPath(pdfPath, options);
      const results = [];
      let actualPages = 0;
      
      for (let i = 1; i <= pageLimit; i++) {
        try {
          const result = await convert(i, { responseType: "image" });
          
          if (result && result.path) {
            results.push(result);
            actualPages = i;
            console.log(`‚úÖ Converted page ${i}: ${result.path}`);
          } else {
            console.log(`üìÑ Reached end of PDF at page ${i-1}`);
            break;
          }
        } catch (error) {
          console.log(`‚ùå Error converting page ${i}:`, error.message);
          if (i === 1) {
            // Try bulk conversion
            try {
              console.log(`üîÑ Trying bulk conversion for all pages...`);
              const bulkResults = await convert.bulk(-1, { responseType: "image" });
              const limitedResults = bulkResults.slice(0, pageLimit);
              
              if (bulkResults.length > pageLimit) {
                console.log(`‚ö†Ô∏è  PDF has ${bulkResults.length} pages, processing only first ${pageLimit} pages`);
                console.log(`üí° To process more pages, set MAX_PDF_PAGES environment variable`);
              }
              
              console.log(`‚úÖ Bulk converted ${limitedResults.length} pages`);
              return limitedResults;
            } catch (bulkError) {
              console.log(`‚ùå Bulk conversion failed:`, bulkError.message);
            }
          }
          break;
        }
      }

      // Log conversion summary
      console.log(`üìä PDF conversion completed: ${results.length} pages processed`);
      if (actualPages === pageLimit) {
        console.log(`‚ö†Ô∏è  Reached page limit (${pageLimit}). PDF might have more pages.`);
        console.log(`üí° To process more pages, set MAX_PDF_PAGES environment variable to higher value`);
      }

      return results;
    } catch (error) {
      console.error('Error converting PDF to images:', error);
      throw error;
    }
  }

  // Extract text from image using Google Cloud Vision
  async extractTextFromImage(imagePath) {
    try {
      console.log(`üîç Extracting text from image using Vision API: ${imagePath}`);
      
      const [result] = await this.visionClient.textDetection(imagePath);
      const detections = result.textAnnotations;
      
      if (!detections || detections.length === 0) {
        console.log('No text detected in image');
        return '';
      }
      
      // First detection contains the full text
      const fullText = detections[0].description;
      
      console.log(`‚úÖ Vision API extracted ${fullText.length} characters`);
      return fullText;
      
    } catch (error) {
      console.error('Error in Vision API OCR:', error);
      return '';
    }
  }

  // Correct OCR text using AI with enhanced Vietnamese support
  async correctOCRText(rawText) {
    try {
      if (!rawText || rawText.trim().length < 10) {
        return rawText;
      }

      console.log(`üîß Correcting OCR text (${rawText.length} characters) with AI...`);
      
      const prompt = `
B·∫°n l√† chuy√™n gia s·ª≠a ch√≠nh t·∫£ v√† c·∫£i thi·ªán vƒÉn b·∫£n ti·∫øng Vi·ªát t·ª´ OCR. Nhi·ªám v·ª•:

NGUY√äN T·∫ÆC:
1. CH·ªà s·ª≠a l·ªói ch√≠nh t·∫£, d·∫•u thanh v√† nh·∫≠n d·∫°ng sai
2. KH√îNG thay ƒë·ªïi n·ªôi dung, √Ω nghƒ©a ho·∫∑c c·∫•u tr√∫c
3. Gi·ªØ nguy√™n format, s·ªë li·ªáu, ng√†y th√°ng
4. S·ª≠a t√™n c√¥ng ty, ch·ª©c v·ª• n·∫øu b·ªã sai
5. C·∫£i thi·ªán kh·∫£ nƒÉng ƒë·ªçc nh∆∞ng gi·ªØ nguy√™n th√¥ng tin

C√ÅC L·ªñI TH∆Ø·ªúNG G·∫∂P:
- D·∫•u thanh ti·∫øng Vi·ªát: "nhan su" ‚Üí "nh√¢n s·ª±"
- Ch·ªØ hoa/th∆∞·ªùng: "CONG TY" ‚Üí "C√îNG TY"
- K√Ω t·ª± ƒë·∫∑c bi·ªát: "¬ß" ‚Üí "ƒêi·ªÅu", "¬¢" ‚Üí "Ch∆∞∆°ng"
- T√™n c√¥ng ty: "PHAT DAT" ‚Üí "PH√ÅT ƒê·∫†T"
- Ch·ª©c v·ª•: "GIAM DOC" ‚Üí "GI√ÅM ƒê·ªêC"

VƒÇN B·∫¢N C·∫¶N S·ª¨A:
${rawText}

VƒÇN B·∫¢N ƒê√É S·ª¨A:`;

      const result = await this.model.generateContent(prompt);
      const response = await result.response;
      let correctedText = response.text().trim();
      
      // Clean up AI response
      if (correctedText.includes('VƒÇN B·∫¢N ƒê√É S·ª¨A:')) {
        correctedText = correctedText.split('VƒÇN B·∫¢N ƒê√É S·ª¨A:')[1].trim();
      }
      
      // Safety check
      if (!correctedText || correctedText.length < rawText.length * 0.3) {
        console.log('‚ö†Ô∏è Correction result too short, using original text');
        return rawText;
      }
      
      console.log(`‚úÖ Text correction completed: ${rawText.length} ‚Üí ${correctedText.length} chars`);
      return correctedText;
      
    } catch (error) {
      console.error('‚ùå Error correcting OCR text:', error);
      return rawText;
    }
  }

  // Classify document content to detect inappropriate or junk files
  async classifyDocumentContent(text, filename = '') {
    try {
      console.log(`üîç Classifying document content...`);
      
      const prompt = `
B·∫°n l√† chuy√™n gia ph√¢n lo·∫°i t√†i li·ªáu doanh nghi·ªáp. Ph√¢n t√≠ch vƒÉn b·∫£n v√† tr·∫£ l·ªùi:

TI√äU CH√ç CH·∫§P NH·∫¨N:
‚úÖ T√†i li·ªáu c√¥ng ty (quy ƒë·ªãnh, quy tr√¨nh, ch√≠nh s√°ch)
‚úÖ B√°o c√°o, bi√™n b·∫£n h·ªçp
‚úÖ H∆∞·ªõng d·∫´n, s∆° ƒë·ªì t·ªï ch·ª©c
‚úÖ H·ª£p ƒë·ªìng, th·ªèa thu·∫≠n
‚úÖ T√†i li·ªáu t√†i ch√≠nh, ki·ªÉm to√°n
‚úÖ VƒÉn b·∫£n ph√°p l√Ω li√™n quan c√¥ng ty

TI√äU CH√ç T·ª™ CH·ªêI:
‚ùå N·ªôi dung kh√¥ng li√™n quan c√¥ng ty
‚ùå T√†i li·ªáu c√° nh√¢n
‚ùå VƒÉn b·∫£n t·∫ßm b·∫≠y, spam
‚ùå N·ªôi dung nh·∫°y c·∫£m
‚ùå File r√°c, test, demo
‚ùå Qu·∫£ng c√°o, marketing kh√¥ng li√™n quan

FILENAME: ${filename}

CONTENT: ${text.substring(0, 2000)}...

Tr·∫£ l·ªùi CH√çNH X√ÅC theo format:
{
  "accept": true/false,
  "category": "Quy ƒë·ªãnh|Quy tr√¨nh|B√°o c√°o|H·ª£p ƒë·ªìng|T√†i ch√≠nh|Kh√°c",
  "confidence": 0.0-1.0,
  "reason": "L√Ω do ng·∫Øn g·ªçn",
  "businessRelevance": "M·ª©c ƒë·ªô li√™n quan c√¥ng ty 0.0-1.0"
}`;

      const result = await this.model.generateContent(prompt);
      const response = await result.response;
      let classificationText = response.text().trim();
      
      // Extract JSON from response
      const jsonMatch = classificationText.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const classification = JSON.parse(jsonMatch[0]);
        console.log(`üìä Document classification:`, classification);
        return classification;
      }
      
      // Fallback classification
      return {
        accept: true,
        category: 'Kh√°c',
        confidence: 0.5,
        reason: 'Unable to classify automatically',
        businessRelevance: 0.5
      };
      
    } catch (error) {
      console.error('‚ùå Error classifying document:', error);
      // Default to accept if classification fails
      return {
        accept: true,
        category: 'Kh√°c',
        confidence: 0.3,
        reason: 'Classification error - defaulting to accept',
        businessRelevance: 0.3
      };
    }
  }

  // Check for duplicate documents using content similarity
  async checkForDuplicates(text, filename, companyId = null) {
    try {
      console.log(`üîç Checking for duplicate documents...`);
      
      // Get existing documents from the same company
      let existingDocs = [];
      if (companyId) {
        // Get all documents and filter by company_id
        const allDocs = await db.getDocuments();
        existingDocs = allDocs.filter(doc => doc.company_id === companyId);
      }
      
      if (existingDocs.length === 0) {
        return { isDuplicate: false, similarDocs: [] };
      }
      
      // Use AI to check similarity
      const prompt = `
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch ƒë·ªô t∆∞∆°ng ƒë·ªìng t√†i li·ªáu. So s√°nh vƒÉn b·∫£n m·ªõi v·ªõi c√°c t√†i li·ªáu hi·ªán c√≥:

VƒÇN B·∫¢N M·ªöI:
Filename: ${filename}
Content: ${text.substring(0, 1500)}...

C√ÅC T√ÄI LI·ªÜU HI·ªÜN C√ì:
${existingDocs.slice(0, 5).map(doc => `
- ID: ${doc.id}
- Filename: ${doc.original_name}
- Content: ${doc.content_text ? doc.content_text.substring(0, 500) : 'No content'}...
`).join('\n')}

Ph√¢n t√≠ch v√† tr·∫£ l·ªùi theo format JSON:
{
  "isDuplicate": true/false,
  "similarDocs": [
    {
      "id": number,
      "filename": "string",
      "similarity": 0.0-1.0,
      "reason": "L√Ω do t∆∞∆°ng ƒë·ªìng"
    }
  ],
  "recommendation": "merge|replace|keep_separate",
  "confidenceScore": 0.0-1.0
}

L∆∞u √Ω:
- similarity > 0.8: Tr√πng l·∫∑p cao
- similarity > 0.6: C√≥ th·ªÉ merge
- similarity < 0.4: Kh√°c bi·ªát, gi·ªØ ri√™ng`;

      const result = await this.model.generateContent(prompt);
      const response = await result.response;
      let analysisText = response.text().trim();
      
      // Extract JSON from response
      const jsonMatch = analysisText.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const analysis = JSON.parse(jsonMatch[0]);
        console.log(`üìä Duplicate analysis:`, analysis);
        return analysis;
      }
      
      return { isDuplicate: false, similarDocs: [] };
      
    } catch (error) {
      console.error('‚ùå Error checking duplicates:', error);
      return { isDuplicate: false, similarDocs: [] };
    }
  }

  // Merge similar documents using AI
  async mergeSimilarDocuments(newText, existingDoc, mergeReason) {
    try {
      console.log(`üîó Merging similar documents...`);
      
      const prompt = `
B·∫°n l√† chuy√™n gia merge t√†i li·ªáu. H√£y k·∫øt h·ª£p 2 t√†i li·ªáu sau th√†nh 1 t√†i li·ªáu ho√†n ch·ªânh:

L√ù DO MERGE: ${mergeReason}

T√ÄI LI·ªÜU HI·ªÜN C√ì:
${existingDoc.content_text}

T√ÄI LI·ªÜU M·ªöI:
${newText}

Y√äU C·∫¶U MERGE:
1. Gi·ªØ th√¥ng tin ƒë·∫ßy ƒë·ªß t·ª´ c·∫£ 2 t√†i li·ªáu
2. Lo·∫°i b·ªè th√¥ng tin tr√πng l·∫∑p
3. S·∫Øp x·∫øp theo logic (th·ªùi gian, m·ª©c ƒë·ªô quan tr·ªçng)
4. ƒê√°nh d·∫•u ngu·ªìn n·∫øu c√≥ th√¥ng tin xung ƒë·ªôt
5. Gi·ªØ format v√† c·∫•u tr√∫c r√µ r√†ng

T√ÄI LI·ªÜU ƒê√É MERGE:`;

      const result = await this.model.generateContent(prompt);
      const response = await result.response;
      let mergedText = response.text().trim();
      
      // Clean up AI response
      if (mergedText.includes('T√ÄI LI·ªÜU ƒê√É MERGE:')) {
        mergedText = mergedText.split('T√ÄI LI·ªÜU ƒê√É MERGE:')[1].trim();
      }
      
      console.log(`‚úÖ Documents merged successfully: ${mergedText.length} characters`);
      return mergedText;
      
    } catch (error) {
      console.error('‚ùå Error merging documents:', error);
      return newText; // Return new text if merge fails
    }
  }

  // Analyze document structure and content for Q&A
  async analyzeDocumentStructure(text) {
    try {
      console.log(`üîç Analyzing document structure for Q&A...`);
      
      const prompt = `
Ph√¢n t√≠ch c·∫•u tr√∫c t√†i li·ªáu ƒë·ªÉ h·ªó tr·ª£ h·ªá th·ªëng h·ªèi ƒë√°p:

CONTENT: ${text.substring(0, 3000)}...

Tr·∫£ l·ªùi theo format JSON:
{
  "documentType": "Quy ƒë·ªãnh|Quy tr√¨nh|B√°o c√°o|H·ª£p ƒë·ªìng|S∆° ƒë·ªì|Kh√°c",
  "mainTopics": ["topic1", "topic2", "topic3"],
  "keyPoints": [
    {
      "section": "Ph·∫ßn n√†o",
      "content": "N·ªôi dung ch√≠nh",
      "importance": 1-5
    }
  ],
  "procedures": [
    {
      "step": 1,
      "description": "B∆∞·ªõc 1",
      "details": "Chi ti·∫øt"
    }
  ],
  "keyTerms": ["term1", "term2"],
  "canAnswerQuestions": [
    "C√≥ th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi g√¨?",
    "Quy tr√¨nh nh∆∞ th·∫ø n√†o?",
    "Ai l√† ng∆∞·ªùi ph·ª• tr√°ch?"
  ]
}`;

      const result = await this.model.generateContent(prompt);
      const response = await result.response;
      let analysisText = response.text().trim();
      
      // Extract JSON from response
      const jsonMatch = analysisText.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const analysis = JSON.parse(jsonMatch[0]);
        console.log(`üìä Document structure analysis completed`);
        return analysis;
      }
      
      return {
        documentType: 'Kh√°c',
        mainTopics: [],
        keyPoints: [],
        procedures: [],
        keyTerms: [],
        canAnswerQuestions: []
      };
      
    } catch (error) {
      console.error('‚ùå Error analyzing document structure:', error);
      return {
        documentType: 'Kh√°c',
        mainTopics: [],
        keyPoints: [],
        procedures: [],
        keyTerms: [],
        canAnswerQuestions: []
      };
    }
  }

  // Process scanned PDF with Vision API
  async processScannedPDF(pdfPath, maxPages = 10) {
    try {
      console.log('üîç Processing scanned PDF with Vision API...');
      
      // Check if we have Vision API credentials
      if (!this.hasValidCredentials()) {
        console.log('‚ö†Ô∏è  Vision API credentials not available, using enhanced fallback');
        return await this.enhancedFallbackProcessing(pdfPath, maxPages);
      }
      
      // Convert PDF to images
      const images = await this.convertPDFToImages(pdfPath, maxPages);
      
      if (images.length === 0) {
        console.log('‚ö†Ô∏è  No images extracted from PDF - may be text-based PDF or conversion failed');
        console.log('üîÑ Attempting enhanced fallback processing...');
        return await this.enhancedFallbackProcessing(pdfPath, maxPages);
      }

      let allText = '';
      
      // Extract text from each image using Vision API
      for (const image of images) {
        const imagePath = image.path;
        const pageText = await this.extractTextFromImage(imagePath);
        
        if (pageText.trim()) {
          allText += `\n--- Trang ${images.indexOf(image) + 1} ---\n`;
          allText += pageText.trim() + '\n';
        }
        
        // Clean up temp image
          fs.unlinkSync(imagePath);
        }

      console.log(`‚úÖ Vision API extraction completed: ${allText.length} characters from ${images.length} pages`);
      return allText.trim();

    } catch (error) {
      console.error('‚ùå Error in processScannedPDF:', error.message);
      
      // Enhanced fallback mechanism
      console.log('üîÑ Attempting enhanced fallback processing...');
      try {
        return await this.enhancedFallbackProcessing(pdfPath, maxPages);
      } catch (fallbackError) {
        console.error('‚ùå Enhanced fallback failed:', fallbackError.message);
        throw new Error(`Vision API failed and enhanced fallback failed: ${error.message}. Original error: ${fallbackError.message}`);
      }
    }
  }

  // Check if we have valid Vision API credentials
  hasValidCredentials() {
    try {
      if (process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON) {
        JSON.parse(process.env.GOOGLE_APPLICATION_CREDENTIALS_JSON);
        return true;
      }
      if (process.env.GOOGLE_APPLICATION_CREDENTIALS && 
          fs.existsSync(process.env.GOOGLE_APPLICATION_CREDENTIALS)) {
        return true;
      }
      return false;
    } catch (error) {
      return false;
    }
  }

  // Enhanced fallback processing that tries multiple methods
  async enhancedFallbackProcessing(pdfPath, maxPages = 10) {
    console.log('üîÑ Starting enhanced fallback processing...');
    
    try {
      // Method 1: Standard PDF parsing
      console.log('üîÑ Method 1: Standard PDF parsing...');
      const pdfParse = require('pdf-parse');
      const dataBuffer = fs.readFileSync(pdfPath);
      const data = await pdfParse(dataBuffer);
      
      if (data.text && data.text.trim().length > 50) {
        console.log(`‚úÖ Standard parsing successful: ${data.text.length} characters`);
        return data.text;
      }
      
      // Method 2: Try Tesseract.js if available and PDF seems to be scanned
      console.log('üîÑ Method 2: Attempting Tesseract.js OCR...');
      return await this.tesseractFallback(pdfPath, Math.min(maxPages, 5));
      
    } catch (error) {
      console.error('‚ùå All enhanced fallback methods failed:', error.message);
      
      // Method 3: Return minimal content with error information
      return `[OCR Processing Failed]
      
This PDF could not be processed automatically because:
- Vision API credentials are not configured
- Standard PDF text extraction yielded minimal results
- Tesseract.js fallback OCR failed

Error: ${error.message}

Please:
1. Configure GOOGLE_APPLICATION_CREDENTIALS_JSON for production
2. Or manually process this scanned document
3. Or contact support for assistance

Document requires manual review.`;
    }
  }

  // Tesseract.js fallback for when Vision API is not available
  async tesseractFallback(pdfPath, maxPages = 5) {
    try {
      // Import Tesseract.js
      const { createWorker } = require('tesseract.js');
      
      // Convert PDF to images with lower quality for fallback
      const images = await this.convertPDFToImages(pdfPath, maxPages);
      
      if (images.length === 0) {
        throw new Error('Could not convert PDF to images for Tesseract processing');
      }
      
      let allText = '';
      const worker = await createWorker();
      
      try {
        // Load Vietnamese and English languages
        await worker.loadLanguage('eng+vie');
        await worker.initialize('eng+vie');
        
        // Process each page (limit to avoid long processing times)
        const pagesToProcess = Math.min(images.length, 3);
        
        for (let i = 0; i < pagesToProcess; i++) {
          const image = images[i];
          console.log(`üîç Processing page ${i + 1} with Tesseract...`);
          
          const { data: { text } } = await worker.recognize(image.path);
          
          if (text.trim()) {
            allText += `\n--- Trang ${i + 1} ---\n`;
            allText += text.trim() + '\n';
          }
          
          // Clean up temp image
          try {
            fs.unlinkSync(image.path);
          } catch (cleanupError) {
            // Ignore cleanup errors
          }
        }
        
        await worker.terminate();
        
        if (allText.trim()) {
          console.log(`‚úÖ Tesseract.js OCR completed: ${allText.length} characters from ${pagesToProcess} pages`);
          return allText.trim();
        } else {
          throw new Error('Tesseract.js extracted no readable text');
        }
        
      } catch (processingError) {
        await worker.terminate();
        throw processingError;
      }
      
    } catch (error) {
      console.error('‚ùå Tesseract.js fallback failed:', error.message);
      throw new Error(`Tesseract OCR failed: ${error.message}`);
    }
  }

  // Enhanced document processing with all new features
  async processDocumentWithEnhancements(pdfPath, filename, originalName, companyId = null) {
    try {
      console.log(`üöÄ Processing document with enhancements: ${originalName}`);
      
      // Step 1: Extract text (standard or OCR)
      const dataBuffer = fs.readFileSync(pdfPath);
      const pdfParse = require('pdf-parse');
      const data = await pdfParse(dataBuffer);
      let extractedText = data.text;
      
      // Use Vision API if standard extraction yields little text
      if (this.isScannedPDF(extractedText)) {
        console.log('üì∏ Using Vision API for scanned document...');
        extractedText = await this.processScannedPDF(pdfPath);
      }
      
      // Step 2: Classify content
      const classification = await this.classifyDocumentContent(extractedText, originalName);
      
      if (!classification.accept) {
        throw new Error(`Document rejected: ${classification.reason}`);
      }
      
      // Step 3: Check for duplicates
      const duplicateAnalysis = await this.checkForDuplicates(extractedText, originalName, companyId);
      
      // Step 4: Handle duplicates if found
      if (duplicateAnalysis.isDuplicate && duplicateAnalysis.recommendation === 'merge') {
        console.log('üîó Merging with similar document...');
        const similarDoc = duplicateAnalysis.similarDocs[0];
        const existingDoc = await db.getDocumentById(similarDoc.id);
        
        if (existingDoc) {
          extractedText = await this.mergeSimilarDocuments(
            extractedText, 
            existingDoc, 
            similarDoc.reason
          );
        }
      }
      
      // Step 5: Analyze document structure
      const structureAnalysis = await this.analyzeDocumentStructure(extractedText);
      
      // Step 6: Cross-document validation and OCR correction (NEW)
      let crossValidationResult = null;
      try {
        // Initialize cross-document validation if not already done
        if (!this.crossDocValidator) {
          const CrossDocumentValidationService = require('../src/services/validation/crossDocumentValidationService');
          this.crossDocValidator = new CrossDocumentValidationService();
        }
        
        console.log('üîÑ Starting cross-document validation and OCR correction...');
        // Note: documentId will be available after document is saved to database
        // This will be called later in the document upload process
        
      } catch (validationError) {
        console.error('‚ö†Ô∏è  Cross-document validation failed, continuing without it:', validationError);
        crossValidationResult = {
          originalText: extractedText,
          correctedText: extractedText,
          entities: {},
          corrections: [],
          conflicts: [],
          confidence: 1.0
        };
      }
      
      return {
        text: extractedText,
        classification: classification,
        duplicateAnalysis: duplicateAnalysis,
        structureAnalysis: structureAnalysis,
        crossValidation: crossValidationResult,
        processingMethod: this.isScannedPDF(data.text) ? 'Vision API OCR' : 'Standard PDF'
      };
      
    } catch (error) {
      console.error('‚ùå Error in enhanced document processing:', error);
      throw error;
    }
  }

  // Cross-document validation and OCR correction - called after document is saved
  async performCrossDocumentValidation(documentId, text, filename, companyId) {
    try {
      console.log(`üîÑ Performing cross-document validation for document ${documentId}`);
      
      // Initialize cross-document validation service if needed
      if (!this.crossDocValidator) {
        const CrossDocumentValidationService = require('../src/services/validation/crossDocumentValidationService');
        this.crossDocValidator = new CrossDocumentValidationService();
      }
      
      // Perform cross-document validation and correction
      const validationResult = await this.crossDocValidator.validateAndCorrectDocument(
        documentId, 
        text, 
        filename, 
        companyId
      );
      
      // Update document with corrected text if significant corrections were made
      if (validationResult.corrections.length > 0 && 
          validationResult.confidence > 0.8 && 
          validationResult.correctedText !== validationResult.originalText) {
        
        console.log(`‚úÖ Applying ${validationResult.corrections.length} corrections to document ${documentId}`);
        
        // Update document content with corrected text
        await db.updateDocument(documentId, {
          content_text: validationResult.correctedText,
          processing_notes: JSON.stringify({
            ocr_corrections: validationResult.corrections.length,
            entity_conflicts: validationResult.conflicts.length,
            validation_confidence: validationResult.confidence,
            processing_timestamp: new Date().toISOString()
          })
        });
        
        console.log(`üìù Updated document ${documentId} with corrected text and metadata`);
      }
      
      // Log validation results
      await this.logValidationResults(documentId, validationResult);
      
      return validationResult;
      
    } catch (error) {
      console.error('‚ùå Error in cross-document validation:', error);
      return {
        originalText: text,
        correctedText: text,
        entities: {},
        corrections: [],
        conflicts: [],
        confidence: 0.5,
        error: error.message
      };
    }
  }

  // Log validation results for debugging and auditing
  async logValidationResults(documentId, validationResult) {
    try {
      // Create tables if they don't exist
      await this.ensureValidationTablesExist();
      
      await db.query(`
        INSERT INTO validation_logs (
          document_id, 
          validation_type, 
          original_text, 
          corrected_text, 
          entities_found, 
          corrections_applied, 
          conflicts_resolved, 
          confidence_score,
          processing_time_ms
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
      `, [
        documentId,
        'cross_document_validation',
        validationResult.originalText ? validationResult.originalText.substring(0, 1000) : '',
        validationResult.correctedText ? validationResult.correctedText.substring(0, 1000) : '',
        JSON.stringify(validationResult.entities || {}),
        JSON.stringify(validationResult.corrections || []),
        JSON.stringify(validationResult.conflicts || []),
        validationResult.confidence || 0.0,
        0 // We'll add timing later if needed
      ]);
      
      console.log(`üìä Logged validation results for document ${documentId}`);
    } catch (error) {
      console.error('‚ùå Error logging validation results:', error);
    }
  }

  // Ensure validation tables exist (for first run)
  async ensureValidationTablesExist() {
    try {
      const checkTablesQuery = `
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = 'public' 
        AND table_name IN ('document_metadata', 'company_metadata', 'validation_logs', 'entity_references')
      `;
      
      const result = await db.query(checkTablesQuery);
      
      if (result.rows.length < 4) {
        console.log('üì¶ Creating validation tables...');
        // Read and execute schema
        const schemaPath = path.join(__dirname, '../scripts/create-metadata-tables.sql');
        if (fs.existsSync(schemaPath)) {
          const schema = fs.readFileSync(schemaPath, 'utf8');
          await db.query(schema);
          console.log('‚úÖ Validation tables created successfully');
        }
      }
    } catch (error) {
      console.error('‚ùå Error ensuring validation tables exist:', error);
    }
  }

  // Clean up temporary files
  cleanup() {
    try {
      if (fs.existsSync(this.tempDir)) {
        const files = fs.readdirSync(this.tempDir);
        files.forEach(file => {
          fs.unlinkSync(path.join(this.tempDir, file));
        });
      }
    } catch (error) {
      console.error('Error cleaning up temp files:', error);
    }
  }
}

module.exports = new VisionOCRService(); 